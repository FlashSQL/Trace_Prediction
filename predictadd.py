# -*- coding: utf-8 -*-
"""test+dahab

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A94BHfyPA7c-8MjyRMU5Up5V6dwdLbFb
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

with open('/content/drive/My Drive/data.txt', 'r') as f:
  lines = f.readlines()
  print(len(lines))
  f.close()

import numpy as np
import torch
import random
from torch.utils.data import TensorDataset, DataLoader
x=[]
y=[]
xarr=[]
yarr=[]
validxarr=[]
validyarr=[]
testxarr=[]
testyarr=[]
seq_size=100
seq_num=1000

for i in range(0,len(lines)):
        lines[i]=lines[i].rstrip()
        lines[i]=int(lines[i])
        lines[i]=lines[i]%1000
#lines=np.hstack((indexarr, lines))
#m=random.randrange(30000000,50000000)
#n= random.randrange(30000000,50000000)
#o= random.randrange(30000000,50000000)
m=40000000
n=40000500
o=40001000

print("m, n, o", m,n,o)
'''
while (m==n):
  n= random.randrange(40000000,60000000)
while (o==n) or (o==m):
  o= random.randrange(40000000,60000000)
'''

for i in range(0,seq_num):
  x=lines[m:m+seq_size]
  xarr.append(x)
  m=m+int(seq_size/2)
  y=lines[m:m+seq_size]
  yarr.append(y)
  m=m+int(seq_size/2)

'''
for j in range(0,20):
  x=lines[n:n+1000]
  validxarr.append(x)
  n=n+500
  y=lines[n:n+1000]
  validyarr.append(y)
  n=n+500
for k in range(0,20):
  x=lines[o:o+1000]
  testxarr.append(x)
  o=o+500
  y=lines[o:o+1000]
  testyarr.append(y)
  o=o+500
'''

tensor_x=torch.LongTensor(xarr)#.cuda()
tensor_y=torch.LongTensor(yarr)#.cuda()
dataset=TensorDataset(tensor_x,tensor_y)
train_loader = DataLoader(dataset, batch_size=1)
#or i in train_loader:
  #print(i)
'''
for data in train_loader:
  print(len(data))
  print("x", data[0])
  print("y", data[1])
'''
'''
valid_tensor_x=torch.Tensor(validxarr).long()
valid_tensor_y=torch.Tensor(validyarr).long()
valid_dataset=TensorDataset(valid_tensor_x,valid_tensor_y)
valid_loader = DataLoader(valid_dataset, batch_size=1)

test_tensor_x=torch.Tensor(testxarr).long()
test_tensor_y=torch.Tensor(testyarr).long()
test_dataset=TensorDataset(test_tensor_x,test_tensor_y)
test_loader = DataLoader(test_dataset, batch_size=1)'''

#LSTM Model 
import torch.nn as nn
from torch import autograd
from torch.nn.utils.rnn import pad_sequence, pack_sequence, pack_padded_sequence, pad_packed_sequence


batch_size=1
num_layers=1
input_size=1000
hidden_size=2048
output_size=seq_size

class Model(nn.Module):
        def __init__(self):
                super(Model,self).__init__()
                self.embedding=nn.Embedding(input_size,hidden_size)
                self.rnn = nn.RNN(hidden_size, hidden_size, nonlinearity='tanh')
        def forward(self,x):
                x=x.permute(1,0)
                self.embedding.zero_grad()
                embedded=self.embedding(x)
                state = autograd.Variable(torch.zeros(1, batch_size, hidden_size)).to(device)
                #print("state size:",state.size())
                out, state = self.rnn(embedded, state)
                #print("out data size:", out.data.shape)
                #print("h_n size:", h_n.size())
                #lstm_out, _ = pad_packed_sequence(packed_output) 
                #print("lstm_out size: ", lstm_out.size())
                out_unembedded= out.view(-1, hidden_size) @ self.embedding.weight.transpose(0,1)
                _, pred= out_unembedded.max(1)
                #out=self.fc(lstm_out)
                #print("out size: ", out.size())
                #print("out[0] before softmax:",out)
                #print("in model final-1 output size: ",out.size())
                #print("out[0] after softmax:",out)
                #print("final out size: ", out.size())
                #print("in model final output size: ",out.size())
                return out_unembedded, pred

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)
#print("This model is running on" , torch.cuda.get_device_name())
#Model
net=Model().to(device)
print(net)
#parameters=[p for p in net.parameters()]+[p for p in embedding.parameters()]
#Get adjustable parameters(weights) and optimize them
#parameters=[p for p in net.parameters()]+[p for p in embedding.parameters()]
optimizer=torch.optim.Adam(net.parameters(),lr=0.00005,weight_decay=0.0001) #weight decay is multiplied co weight to prevent them from growing too large
#Error Function
criterion = torch.nn.functional.nll_loss
# Learning rate scheduler: adjusts learning rate as the epoch increases
exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1) #Decays the learning rate by multiplyin by gamma every step_size epochs
#How many times we pass our full data (the same data)
total_epoch=50000

#Training and Validation
best_valid_acc=0
checkarr=[]
checkarrcheck=0
accarr=[]
net.train()
for i in range(seq_num):#same with batch num
  checkarr.append(0)
  accarr.append(0)
for cur_epoch in range(total_epoch):
  for p,data in enumerate(train_loader):
    X,y=data[0].to(device), data[1].to(device)
    output, pred=net(X)
    check=0
    loss = criterion(output, y.view(-1))
    darr=[]
    for i in range(seq_size):#same with batch size
      if pred[i]==y.view(-1)[i]:
        check=1
      else:
        darr.append(i)
    accarr[p]=seq_size-len(darr)
    #print("different:",darr)
    #print("batch",p)
    #if cur_epoch % 100==0:
      #print("len of different",len(darr))
    optimizer.zero_grad()  
    net.zero_grad()
    loss.backward()
    optimizer.step()
  #exp_lr_scheduler.step()
    #train_total +=1  
  if (cur_epoch % 10==0 and cur_epoch!=0):
    print('epoch',cur_epoch)
    print("loss: ",loss)
    print("accarr",accarr)
    sum=0
    acc=0
    for i in range(seq_num):
      sum=sum+accarr[i]
    acc=sum/seq_num
    print(acc)
    #print("pred",pred)
    #print("target",y.view(-1))

  '''net.eval()
  with torch.no_grad():
    for data in valid_loader:
      X,y =data[0].to(device), data[1].to(device)
      output, pred = net(X)
      #y=y.permute(1,0)
      #output=output.view(output_size, input_size, -1)
      #output_unembedded=output.view(-1, hidden_size) @ embedding.weight.transpose(1,0)
      loss = criterion(output,y.view(-1))
      valid_loss+=loss.item()
      check=0
      for k,i in enumerate(pred):
        if pred[k]==y.view(-1)[k]:
          check=1
        else:
          check=0
          break
      if check==1:
        valid_correct+=1
      valid_total +=1
  if((valid_correct/valid_total)>best_valid_acc):
    best_valid_acc=(valid_correct/valid_total)
    torch.save(net.state_dict(), "./save_best.pth")
  if((cur_epoch+1)%(total_epoch*0.1)==0):
    print(' Epoch {}/{}: Training Accuracy {} |  Training Loss {} || Validation Accuracy {} |  Validation Loss {}'.format(cur_epoch+1, total_epoch, train_correct/train_total,train_loss/len(train_loader),valid_correct/valid_total,valid_loss/len(valid_loader))) #accuray for each epoch
    print(' Best validation so far {}'.format(best_valid_acc))
    print('-------------------------------------------------------------------------------------------------------------------------------')'''

load_model = Model().to(device)
load_model.load_state_dict(torch.load("./save_best.pth")) 

load_model.eval()

correct =0
total=0
with torch.no_grad(): # no gradient
  for data in test_loader:
      X, y = data[0].to(device), data[1].to(device) # store the Xs and labels
      output = load_model(X, lengths) 
      y=y.permute(1,0)
      for k, i in enumerate(output): # 
        if torch.argmax(i) == y[k]: # in every row find the highest prediction and comprae its index 
          correct += 1
        total += 1

print("Test Accuracy: ", correct/total)